
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Distributional TD learning &#8212; Juliaで学ぶ計算論的神経科学</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script src="../_static/custom.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://compneuro-julia.github.io/bayesian-brain/distributional-td-learning.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-172979897-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Juliaで学ぶ計算論的神経科学</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Juliaで学ぶ計算論的神経科学
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../preface.html">
   まえがき
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../introduction/intro.html">
   はじめに
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../introduction/notation.html">
     記号の表記
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../neuron-model/intro.html">
   神経細胞のモデル
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuron-model/hodgkin-huxley.html">
     Hodgkin-Huxleyモデル
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuron-model/fhn.html">
     FitzHugh-Nagumoモデル
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuron-model/lif.html">
     Leaky integrate-and-fire モデル
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuron-model/izhikevich.html">
     Izhikevich モデル
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuron-model/isi.html">
     Inter-spike interval モデル
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../synapse-model/intro.html">
   シナプス伝達のモデル
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../synapse-model/synapse.html">
     シナプス伝達
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../synapse-model/current-conductance-synapse.html">
     Current / Conductance-based シナプス
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../synapse-model/expo-synapse.html">
     指数関数型シナプスモデル
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../synapse-model/kinetic-synapse.html">
     動力学モデル
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../synapse-model/synaptic-weighted.html">
     シナプス入力の重みづけ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../synapse-model/dynamical-synapses.html">
     動的シナプス
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../neuronal-computation/intro.html">
   神経回路網の演算処理
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuronal-computation/neuronal-arithmetic.html">
     ゲイン調節と四則演算
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../learning-rule/intro.html">
   神経回路網の学習則
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../learning-rule/hebbian-learning.html">
     Hebb則と教師なし学習
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../learning-rule/backpropagation-zipser-andersen.html">
     勾配法と誤差逆伝播法（Zipser-Andersenモデルを例にして）
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../learning-rule/bptt.html">
     BPTT (backpropagation through time)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../learning-rule/linear-network-learning-dynamics.html">
     線形多層ニューラルネットワークの学習ダイナミクス
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../information-theory/intro.html">
   情報理論と教師なし・自己教師あり学習
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../information-theory/statistics-information.html">
     統計と情報理論の基礎
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../information-theory/slow-feature-analysis.html">
     Slow Feature Analysis (SFA)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../associative-memory-model/intro.html">
   連想記憶モデル
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../associative-memory-model/energy-based-model.html">
     エネルギーベースモデル (Energy-based model)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../associative-memory-model/amari-hopfield-model.html">
     Amari-Hopfield モデル
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../associative-memory-model/boltzmann-machine.html">
     Boltzmann マシン
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="intro.html">
   ベイズ脳仮説と生成モデル
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="bayes-statistics.html">
     ベイズ統計の基礎
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linear-regression.html">
     線形回帰と最小二乗法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparse-coding.html">
     スパース符号化 (sparse coding)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="predictive-coding.html">
     予測符号化 (predictive coding)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neural-uncertainty-representation.html">
     ベイズ脳仮説と神経活動による不確実性の表現
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bayesian-linear-regression.html">
     ベイズ線形回帰 (Bayesian linear regression)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mcmc.html">
     マルコフ連鎖モンテカルロ法 (MCMC)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neural-sampling.html">
     神経サンプリング
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="probabilistic-population-coding.html">
     確率的集団符号化 (probabilistic population coding)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="quantile-expectile-regression.html">
     分位点回帰とエクスペクタイル回帰
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../reinforcement-learning/intro.html">
   強化学習
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement-learning/td-learning.html">
     TD学習
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../motor-learning/intro.html">
   運動制御
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../motor-learning/minimum-jerk.html">
     躍度最小モデル
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../motor-learning/minimum-variance.html">
     終点誤差分散最小モデル (minimum-variance model)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../motor-learning/optimal-feedback-control.html">
     最適フィードバック制御モデル (optimal feedback control; OFC)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../motor-learning/infinite-horizon-ofc.html">
     無限時間最適制御モデル (infinite-horizon optimal feedback control model)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../spatiotemporal-coding/intro.html">
   時空間の符号化
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../spatiotemporal-coding/grid-cells-decoding.html">
     格子細胞のデコーディング
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../neural-development/intro.html">
   神経回路の形態・発生の数理モデル
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../neural-development/neurite-growth-model.html">
     神経突起の成長モデル
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neural-development/self-organizing-map.html">
     自己組織化マップと視覚野の構造
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neural-development/graph-theory-network-model.html">
     グラフ理論とネットワークモデル
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../appendix/intro.html">
   付録
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../appendix/rat-trajectory.html">
     ラット自由行動下の軌跡のシミュレーション
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../appendix/tips.html">
     JuliaのTips集
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../appendix/useful-links.html">
     有用なリンク集
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../appendix/usage-jupyter-book.html">
     Jupyter bookの使い方 (Julia言語版)
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/compneuro-julia/compneuro-julia-management/master?urlpath=lab/tree/contents/bayesian-brain/distributional-td-learning.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/compneuro-julia/compneuro-julia-management/blob/master/contents/bayesian-brain/distributional-td-learning.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/compneuro-julia/compneuro-julia-management"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/compneuro-julia/compneuro-julia-management/edit/master/contents/bayesian-brain/distributional-td-learning.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/bayesian-brain/distributional-td-learning.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Distributional TD learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quantile">
   12.2.2 分位数(Quantile)モデルと報酬分布の符号化
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rpesign">
     RPEに対する応答がsign関数のモデルと報酬分布の分位点への予測価値の収束
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#signdistributional-rl">
     sign関数を用いたDistributional RLと分位点回帰
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#expectile-decoding">
   12.2.3 Expectile モデルとドパミンニューロンからの報酬分布のDecoding
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rpeexpectile">
     RPEに対する応答が線形なモデルとExpectile回帰
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decoding">
     報酬分布のデコーディング (decoding)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   参考文献
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Distributional TD learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Distributional TD learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quantile">
   12.2.2 分位数(Quantile)モデルと報酬分布の符号化
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rpesign">
     RPEに対する応答がsign関数のモデルと報酬分布の分位点への予測価値の収束
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#signdistributional-rl">
     sign関数を用いたDistributional RLと分位点回帰
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#expectile-decoding">
   12.2.3 Expectile モデルとドパミンニューロンからの報酬分布のDecoding
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rpeexpectile">
     RPEに対する応答が線形なモデルとExpectile回帰
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decoding">
     報酬分布のデコーディング (decoding)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   参考文献
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="distributional-td-learning">
<h1>Distributional TD learning<a class="headerlink" href="#distributional-td-learning" title="Permalink to this headline">#</a></h1>
<ul class="simple">
<li><p>Quantileはノンパラ</p></li>
<li><p>PPCやDPCはパラメトリック</p></li>
</ul>
<p>Distributional Reinforcement Learning in the Brainに</p>
<blockquote>
<div><p>Quantile-like codes are non-parametric codes, as they do not a priori assume a specific form of a probability distribution with associated parameters. Previous studies have proposed different population coding schemes. For example, probabilistic population codes (PPCs) [73,74] and distributed distributional codes (DDCs) [75,76] employ population coding schemes from which various statistical parameters of a distribution can be read out, making them parametric codes. As a simple example, a PPC might encode a Gaussian distribution, in which case the mean would be reflected in which specific neurons are most active, and the variance would be reflected in the inverse of the overall activity [73].</p>
</div></blockquote>
<p>(<a class="reference external" href="https://www.nature.com/articles/s41586-019-1924-6">Dabney, et al., <em>Nature.</em> 2020</a>)における<strong>Distributional Reinforcement Learning</strong>について取り扱う．コード(MATLAB, Python)も公開されている(<a class="reference external" href="https://doi.org/10.17605/OSF.IO/UX5RG">https://doi.org/10.17605/OSF.IO/UX5RG</a>) ．</p>
<div class="section" id="id1">
<h2>Distributional TD learning<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h2>
<p>Distributional TD learningではRPEの正負に応じて，予測報酬の更新を異なる学習率(<span class="math notranslate nohighlight">\(\alpha_{i}^{+}, \alpha_{i}^{-}\)</span>)を用いて行う．</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
\begin{cases} V_{i}(x) \leftarrow V_{i}(x)+\alpha_{i}^{+} f\left(\delta_{i}\right) &amp;\text{for }
\delta_{i} \gt 0\\ V_{i}(x) \leftarrow V_{i}(x)+\alpha_{i}^{-} f\left(\delta_{i}\right) &amp;\text{for } \delta_{i} \leq 0 \end{cases} 
\end{split}\]</div>
<p>ここで，シミュレーションにおいては<span class="math notranslate nohighlight">\(\alpha_{i}^{+}, \alpha_{i}^{-}\sim U(0,
1)\)</span>とする(<span class="math notranslate nohighlight">\(U\)</span>は一様分布)．さらにasymmetric scaling factor <span class="math notranslate nohighlight">\(\tau_i\)</span>を次式により定義する．</p>
<div class="math notranslate nohighlight">
\[ 
\tau_i=\frac{\alpha_{i}^{+}}{\alpha_{i}^{+}+ \alpha_{i}^{-}} 
\]</div>
<p>なお，<span class="math notranslate nohighlight">\(\alpha_{i}^{+}, \alpha_{i}^{-}\in [0, 1]\)</span>より<span class="math notranslate nohighlight">\(\tau_i \in
[0,1]\)</span>である．</p>
<p>Classical TD learningとDistributional TD learningにおける各ニューロンのRPEに対する発火率を表現したのが次図となる．</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span> <span class="n">PyPlot</span><span class="p">,</span> <span class="n">StatsBase</span>
<span class="n">rc</span><span class="p">(</span><span class="s">&quot;axes.spines&quot;</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># Classical TD learning</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">get_cmap</span><span class="p">(</span><span class="s">&quot;brg&quot;</span><span class="p">)</span> 
<span class="n">cmap</span> <span class="o">=</span> <span class="n">c</span><span class="p">(</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">N</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">range</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
<span class="n">θ</span> <span class="o">=</span> <span class="n">range</span><span class="p">(</span><span class="nb">π</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span> <span class="nb">π</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">α</span> <span class="o">=</span> <span class="n">tan</span><span class="o">.</span><span class="p">(</span><span class="n">θ</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">α</span> <span class="o">*</span> <span class="n">x</span><span class="o">&#39;</span>

<span class="c"># Plot</span>
<span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&quot;gray&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&quot;gray&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">N</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="kt">Int</span><span class="p">(</span><span class="n">N</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>       
        <span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="o">:</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">[</span><span class="kt">Int</span><span class="p">(</span><span class="n">N</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="o">:</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;Neutral&quot;</span><span class="p">)</span>
    <span class="k">else</span>
        <span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="o">:</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">[</span><span class="kt">Int</span><span class="p">(</span><span class="n">N</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="o">:</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="k">end</span>
<span class="k">end</span>

<span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span> <span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xticks</span><span class="p">([]);</span> <span class="n">yticks</span><span class="p">([])</span>
<span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">title</span><span class="p">(</span><span class="s">&quot;Classical TD learning&quot;</span><span class="p">)</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;RPE&quot;</span><span class="p">)</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;Firing&quot;</span><span class="p">)</span>

<span class="c"># Distributional TD learning</span>
<span class="n">α_pos</span> <span class="o">=</span> <span class="n">tan</span><span class="o">.</span><span class="p">(</span><span class="n">θ</span><span class="p">)</span>
<span class="n">α_neg</span> <span class="o">=</span> <span class="n">reverse!</span><span class="p">(</span><span class="n">tan</span><span class="o">.</span><span class="p">(</span><span class="n">θ</span><span class="p">))</span>
 
<span class="n">yd</span> <span class="o">=</span> <span class="p">(</span><span class="n">α_pos</span> <span class="o">*</span> <span class="p">((</span><span class="n">x</span> <span class="o">.&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">.*</span> <span class="n">x</span><span class="p">)</span><span class="o">&#39;</span> <span class="o">+</span> <span class="p">(</span><span class="n">α_neg</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">x</span> <span class="o">.≤</span> <span class="mi">0</span><span class="p">)</span> <span class="o">.*</span> <span class="n">x</span><span class="p">)</span><span class="o">&#39;</span><span class="p">)</span> 

<span class="c"># Plot</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&quot;gray&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&quot;gray&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">N</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span>        
        <span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yd</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="o">:</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="o">:</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">label</span><span class="o">=</span><span class="s">&quot;Pessimistic&quot;</span><span class="p">)</span>
    <span class="k">elseif</span> <span class="n">i</span> <span class="o">==</span> <span class="kt">Int</span><span class="p">(</span><span class="n">N</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>       
        <span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yd</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="o">:</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="o">:</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">label</span><span class="o">=</span><span class="s">&quot;Neutral&quot;</span><span class="p">)</span>
    <span class="k">elseif</span> <span class="n">i</span> <span class="o">==</span> <span class="n">N</span>
        <span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yd</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="o">:</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="o">:</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">label</span><span class="o">=</span><span class="s">&quot;Optimistic&quot;</span><span class="p">)</span>
    <span class="k">else</span>
        <span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yd</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="o">:</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="o">:</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="k">end</span>
<span class="k">end</span>
<span class="n">handles</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">reverse!</span><span class="p">(</span><span class="n">handles</span><span class="p">),</span> <span class="n">reverse!</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span> <span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span> <span class="n">xticks</span><span class="p">([]);</span> <span class="n">yticks</span><span class="p">([])</span>
<span class="n">title</span><span class="p">(</span><span class="s">&quot;Distributional TD learning&quot;</span><span class="p">);</span> <span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;RPE&quot;</span><span class="p">);</span> <span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;Firing&quot;</span><span class="p">)</span>
<span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/distributional-td-learning_5_0.png" src="../_images/distributional-td-learning_5_0.png" />
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>┌ Warning: `vendor()` is deprecated, use `BLAS.get_config()` and inspect the output instead
│   caller = npyinitialize() at numpy.jl:67
└ @ PyCall C:\Users\yamtak\.julia\packages\PyCall\3fwVL\src\numpy.jl:67
</pre></div>
</div>
</div>
</div>
<p>Classical TD learningではRPEに比例して発火する細胞しかないが，Distributional TD learningではRPEの正負に応じて発火率応答が変化していることがわかる． 特に<span class="math notranslate nohighlight">\(\alpha_{i}^{+} \gt \alpha_{i}^{-}\)</span>の細胞を<strong>楽観的細胞 (optimistic cells)</strong>，<span class="math notranslate nohighlight">\(\alpha_{i}^{+}\lt
\alpha_{i}^{-}\)</span>の細胞を<strong>悲観的細胞 (pessimistic
cells)</strong> と著者らは呼んでいる．実際には2群に分かれているわけではなく，gradientに遷移している．楽観的・悲観的の意味に関しては後でも触れるが，ここではイメージだけお伝えしておこう．まず楽観的細胞ではRPEが正なら「結構もらえる」，RPEが負なら「まあそういうときもある」となり最終的な予測価値は通常よりも高くなる．逆に悲観的細胞ではRPEが正なら「もらえたけどいつもそうではない…」，RPEが負なら「やっぱあんまりもらえないよね」となり最終的な予測価値は通常よりも低くなる．収束する予測価値が細胞ごとに異なることで，<span class="math notranslate nohighlight">\(V\)</span>には報酬の期待値ではなく複雑な形状の報酬分布が符号化される．その仕組みについて，次項から見ていこう．</p>
</div>
<div class="section" id="quantile">
<h2>12.2.2 分位数(Quantile)モデルと報酬分布の符号化<a class="headerlink" href="#quantile" title="Permalink to this headline">#</a></h2>
<div class="section" id="rpesign">
<h3>RPEに対する応答がsign関数のモデルと報酬分布の分位点への予測価値の収束<a class="headerlink" href="#rpesign" title="Permalink to this headline">#</a></h3>
<p>さて，Distributional RLモデルでどのようにして報酬分布が学習されるかについてみていこう．この項ではRPEに対する応答関数<span class="math notranslate nohighlight">\(f(\cdot)\)</span>が符合関数(sign function)の場合を考える．結論から言うと，この場合はasymmetric scaling factor <span class="math notranslate nohighlight">\(\tau_i\)</span>は分位数(quantile)となり，<strong>予測価値
<span class="math notranslate nohighlight">\(V_i\)</span>は報酬分布の<span class="math notranslate nohighlight">\(\tau_i\)</span>分位数に収束する</strong>．</p>
<p>どういうことかを簡単なシミュレーションで見てみよう．今，報酬分布を平均2, 標準偏差5の正規分布とする (すなわち<span class="math notranslate nohighlight">\(r \sim N(2, 5^2)\)</span>となります)．また，<span class="math notranslate nohighlight">\(\tau_i = 0.25, 0.5, 0.75 (i=1,2,3)\)</span>とする．このとき，3つの予測価値 <span class="math notranslate nohighlight">\(V_i \ (i=1,2,3)\)</span>はそれぞれ<span class="math notranslate nohighlight">\(N(2, 5^2)\)</span>の0.25, 0.5,
0.75分位数に収束する．下図はシミュレーションの結果である．左が<span class="math notranslate nohighlight">\(V_i\)</span>の変化で，右が報酬分布と0.25, 0.5, 0.75分位数の位置 (黒短線)となっています．対応する分位数に見事に収束していることが分かる．</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># init </span>
<span class="n">response_func</span><span class="p">(</span><span class="n">r</span><span class="p">)</span> <span class="o">=</span> <span class="n">sign</span><span class="p">(</span><span class="n">r</span><span class="p">)</span> <span class="c"># RPEの応答関数</span>
 
<span class="n">num_cells</span> <span class="o">=</span> <span class="mi">3</span> <span class="c"># ニューロン(ユニット)の数</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">5000</span> <span class="c"># 訓練回数</span>
<span class="n">base_lr</span> <span class="o">=</span> <span class="mf">0.02</span> <span class="c"># ベースラインの学習率(learning rate)</span>
 
<span class="n">μreward</span> <span class="o">=</span> <span class="mi">5</span> <span class="c"># 報酬の平均(正規分布)</span>
<span class="n">σreward</span> <span class="o">=</span> <span class="mi">2</span> <span class="c"># 報酬の標準偏差(正規分布)</span>
 
<span class="n">distribution</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">num_cells</span><span class="p">)</span> <span class="c"># 価値分布を記録する配列</span>
<span class="n">dist_trans</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">num_steps</span><span class="p">,</span> <span class="n">num_cells</span><span class="p">)</span> <span class="c"># 価値分布を記録する配列</span>
 
<span class="n">α₊</span><span class="p">,</span> <span class="n">α₋</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span> <span class="c"># RPEが正, 負のときの学習率</span>
<span class="n">τ</span> <span class="o">=</span> <span class="n">α₊</span> <span class="o">./</span> <span class="p">(</span><span class="n">α₊</span> <span class="o">+</span> <span class="n">α₋</span><span class="p">);</span> <span class="c"># Asymmetric scaling factor</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># simulation</span>
<span class="k">for</span> <span class="n">step</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">num_steps</span> <span class="c"># 25000 steps</span>
    <span class="c"># 報酬がrandomに選ばれる</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">μreward</span> <span class="o">+</span> <span class="n">randn</span><span class="p">()</span><span class="o">*</span><span class="n">σreward</span>
     
    <span class="c"># 報酬誤差(step毎に更新) reward応答をlinearとする</span>
    <span class="n">δ</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">.-</span> <span class="n">distribution</span> <span class="c"># (3, )</span>
 
    <span class="c"># δが負なら1, 正なら0</span>
    <span class="n">valence</span> <span class="o">=</span> <span class="n">δ</span> <span class="o">.≤</span> <span class="mi">0</span> <span class="c"># (3, )</span>
 
    <span class="c"># 予測価値分布の更新</span>
    <span class="n">α</span> <span class="o">=</span> <span class="n">valence</span> <span class="o">.*</span> <span class="n">α₋</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">.-</span> <span class="n">valence</span><span class="p">)</span> <span class="o">.*</span> <span class="n">α₊</span>
    <span class="n">distribution</span> <span class="o">+=</span> <span class="n">α</span> <span class="o">.*</span> <span class="n">response_func</span><span class="o">.</span><span class="p">(</span><span class="n">δ</span><span class="p">)</span> <span class="o">.*</span> <span class="n">base_lr</span>
    <span class="n">dist_trans</span><span class="p">[</span><span class="n">step</span><span class="p">,</span> <span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">distribution</span> <span class="c"># 予測価値分布変化の記録</span>
<span class="k">end</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># Results plot</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="n">num_steps</span>
<span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c"># 予測価値の変化</span>

<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">num_cells</span>   
    <span class="n">plot</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="n">dist_trans</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="p">(</span><span class="n">string</span><span class="p">((</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">25</span><span class="p">)</span><span class="o">*</span><span class="s">&quot;%tile (&quot;</span><span class="o">*</span><span class="sa">L</span><span class="s">&quot;$</span><span class="se">\t</span><span class="s">au=$&quot;</span><span class="o">*</span><span class="n">string</span><span class="p">((</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mf">0.25</span><span class="p">)</span><span class="o">*</span><span class="s">&quot;)&quot;</span><span class="p">))</span>
<span class="k">end</span>

<span class="n">title</span><span class="p">(</span><span class="s">&quot;Convergence of value prediction to </span><span class="se">\n</span><span class="s"> percentile of reward distribution&quot;</span><span class="p">)</span>
<span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">)</span>
<span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;Learning steps&quot;</span><span class="p">)</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;Learned Value&quot;</span><span class="p">)</span>
<span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/distributional-td-learning_10_0.png" src="../_images/distributional-td-learning_10_0.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>PyObject &lt;matplotlib.legend.Legend object at 0x0000000000EA9FD0&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># Gaussian kernel density estimation</span>
<span class="k">function</span> <span class="n">kde</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dx</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">band_width</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">minimum</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">:</span><span class="n">dx</span><span class="o">:</span><span class="n">maximum</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">zero</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">size</span><span class="p">(</span><span class="n">data</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">n</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="n">exp</span><span class="o">.</span><span class="p">(</span><span class="o">-</span><span class="p">(((</span><span class="n">x</span> <span class="o">.-</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">/</span><span class="n">band_width</span><span class="p">)</span><span class="o">.^</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">end</span>
    <span class="n">y</span> <span class="o">/=</span> <span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">band_width</span><span class="o">*</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="nb">π</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
<span class="k">end</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>kde (generic function with 4 methods)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># 報酬のサンプリング</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="n">μreward</span> <span class="o">.+</span> <span class="n">randn</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span> <span class="o">*</span> <span class="n">σreward</span>
<span class="n">qtile</span> <span class="o">=</span> <span class="n">nquantile</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="mi">3</span><span class="p">);</span> <span class="c"># 報酬の四分位数を取得</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">kde</span><span class="p">(</span><span class="n">rewards</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">hist</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="nb">true</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s">&quot;vertical&quot;</span><span class="p">)</span>
<span class="n">title</span><span class="p">(</span><span class="s">&quot;Reward</span><span class="se">\n</span><span class="s"> distribution&quot;</span><span class="p">)</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;Density&quot;</span><span class="p">)</span>
<span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/distributional-td-learning_13_0.png" src="../_images/distributional-td-learning_13_0.png" />
</div>
</div>
<p>ここでoptimisticな細胞(<span class="math notranslate nohighlight">\(\tau=0.75\)</span>)は中央値よりも高い予測価値，pessimisticな細胞(<span class="math notranslate nohighlight">\(\tau=0.25\)</span>)は中央値よりも低い予測価値に収束しています． つまり細胞の楽観度というものは，細胞が期待する報酬が大きいほど上がります．</p>
<p>同様のシミュレーションを今度は200個の細胞 (ユニット)で行います．報酬は0.1, 1, 2 μLのジュースがそれぞれ確率0.3, 0.6, 0.1で出るとします (Extended Data Fig.1と同じような分布にしています)．なお，著者らはシミュレーションとマウスに対して<b>Variable-magnitude task</b>
(異なる量の報酬(ジュース)が異なる確率で出る)と<b>Variable-probability task</b> (一定量の報酬がある確率で出る)を行っています．以下はVariable-magnitude taskを行う，ということです．学習結果は次図のようになります．左はGround Truthの報酬分布で，右は<span class="math notranslate nohighlight">\(V_i\)</span>に対してカーネル密度推定
(KDE)することによって得た予測価値分布です．2つの分布はほぼ一致していることが分かります．</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">response_func</span><span class="p">(</span><span class="n">r</span><span class="p">)</span> <span class="o">=</span> <span class="n">sign</span><span class="o">.</span><span class="p">(</span><span class="n">r</span><span class="p">)</span> <span class="c"># RPEの応答関数</span>
 
<span class="n">juice_amounts</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="c"># reward(ジュース)の量(uL)</span>
<span class="n">juice_probs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span> <span class="c"># 各ジュースが出る確率</span>

<span class="n">num_cells</span> <span class="o">=</span> <span class="mi">200</span> <span class="c"># ニューロン(ユニット)の数</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">25000</span> <span class="c"># 訓練回数</span>
<span class="n">base_lrate</span> <span class="o">=</span> <span class="mf">0.02</span> <span class="c"># ベースラインの学習率</span>
   
<span class="n">distribution</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">num_cells</span><span class="p">)</span> <span class="c"># 価値分布を記録する配列</span>

<span class="n">α₊</span><span class="p">,</span> <span class="n">α₋</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">num_cells</span><span class="p">),</span> <span class="n">rand</span><span class="p">(</span><span class="n">num_cells</span><span class="p">)</span> <span class="c"># RPEが正, 負のときの学習率</span>
<span class="n">τ</span> <span class="o">=</span> <span class="n">α₊</span> <span class="o">./</span> <span class="p">(</span><span class="n">α₊</span> <span class="o">+</span> <span class="n">α₋</span><span class="p">);</span> <span class="c"># Asymmetric scaling factor</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>200-element Vector{Float64}:
 0.25440420500745925
 0.7852155812139469
 0.6594394473977122
 0.3484773745277892
 0.08171035938443488
 0.5675062802659787
 0.4933286867244028
 0.4579509059770634
 0.836422443639698
 0.17634813815403
 0.611315137425349
 0.36208217053384356
 0.32466122213479776
 ⋮
 0.2284948827523684
 0.4977506608152836
 0.42995559263235394
 0.9324188446113607
 0.4958919089249423
 0.640575252891557
 0.2323518916624266
 0.7174430117353767
 0.22481036149758837
 0.9113207308685322
 0.6930908334057383
 0.22325509627759538
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">step</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">num_steps</span> <span class="c"># 25000 steps</span>
    <span class="c"># 報酬がrandomに選ばれる</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">juice_amounts</span><span class="p">,</span> <span class="n">pweights</span><span class="p">(</span><span class="n">juice_probs</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span> <span class="c">#(1, ) StatsBase.jl参照</span>
     
    <span class="c"># 報酬誤差(step毎に更新) reward応答をlinearとする</span>
    <span class="n">δ</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">.-</span> <span class="n">distribution</span> <span class="c"># (200, )</span>
 
    <span class="c"># deltaが負なら1, 正なら0</span>
    <span class="n">valence</span> <span class="o">=</span> <span class="n">δ</span> <span class="o">.≤</span> <span class="mi">0</span> <span class="c"># (200, )</span>
 
    <span class="c"># 予測価値分布の更新</span>
    <span class="n">α</span> <span class="o">=</span> <span class="n">valence</span> <span class="o">.*</span> <span class="n">α₋</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">.-</span> <span class="n">valence</span><span class="p">)</span> <span class="o">.*</span> <span class="n">α₊</span>
    <span class="n">distribution</span> <span class="o">+=</span> <span class="n">α</span> <span class="o">.*</span> <span class="n">response_func</span><span class="o">.</span><span class="p">(</span><span class="n">δ</span><span class="p">)</span> <span class="o">*</span> <span class="n">base_lrate</span>
<span class="k">end</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># τの大きさでソートする</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">sortperm</span><span class="p">(</span><span class="n">τ</span><span class="p">)</span>
<span class="n">τ</span> <span class="o">=</span> <span class="n">τ</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span class="n">α₊</span> <span class="o">=</span> <span class="n">α₊</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span class="n">α₋</span> <span class="o">=</span> <span class="n">α₋</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span class="n">distribution</span> <span class="o">=</span> <span class="n">distribution</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>200-element Vector{Float64}:
 0.09501049742703858
 0.09218325100436639
 0.09681031483727352
 0.0914594596775125
 0.09769024764689965
 0.10223071428183231
 0.10182241041022282
 0.10419987711702916
 0.10416524973426801
 0.09919498777141457
 0.09397998112467998
 0.1001769073451679
 0.10380400898982714
 ⋮
 1.551295237261347
 1.9821735133786753
 1.97203898774334
 1.9979014381603757
 1.9977378061741913
 2.0081984176457675
 2.0001241279730646
 1.9999422174396626
 2.0135340538273767
 2.00546972205919
 2.009774967067564
 2.0007043189567537
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># 報酬をサンプリング</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">juice_amounts</span><span class="p">,</span> <span class="n">pweights</span><span class="p">(</span><span class="n">juice_probs</span><span class="p">),</span> <span class="mi">1000</span><span class="p">)</span>
 
<span class="c"># 結果の描画(価値・報酬分布)</span>
<span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c"># Ground Truth (Reward分布)</span>
<span class="n">title</span><span class="p">(</span><span class="s">&quot;Reward distribution&quot;</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">kde</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">);</span>
<span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">);</span> <span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">zero</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">scatter</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">zero</span><span class="p">(</span><span class="n">rewards</span><span class="p">),</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">&quot;|&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&quot;k&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;Reward&quot;</span><span class="p">)</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;Density&quot;</span><span class="p">)</span>
 
<span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c"># 学習後のValue(Reward)の分布</span>
<span class="n">title</span><span class="p">(</span><span class="s">&quot;Learned Value distribution&quot;</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">kde</span><span class="p">(</span><span class="n">distribution</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">);</span>
<span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">);</span> <span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">zero</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">scatter</span><span class="p">(</span><span class="n">distribution</span><span class="p">,</span> <span class="n">zero</span><span class="p">(</span><span class="n">distribution</span><span class="p">),</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">&quot;|&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&quot;k&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="c"># rugplot</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;Value&quot;</span><span class="p">)</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;Density&quot;</span><span class="p">)</span>
<span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/distributional-td-learning_18_0.png" src="../_images/distributional-td-learning_18_0.png" />
</div>
</div>
<p>そして<span class="math notranslate nohighlight">\(V_i\)</span>の経験累積分布関数(CDF)は<span class="math notranslate nohighlight">\(r\)</span>のサンプリングしたCDFとほぼ同一となっています (下図左)．また，<span class="math notranslate nohighlight">\(\tau_i\)</span>の関数である<span class="math notranslate nohighlight">\(V_i\)</span>は<b>分位点関数 (quantile function)</b>または累積分布関数の逆関数 (inverse cumulative distribution function)となっています
(下図右)．右の図を転置すると左の青い曲線とだいたい一致しそうなことが分かります．</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span># 結果の描画(累積分布)
figure(figsize=(8,4))
subplot(1,2,1) # 累積分布
sns.kdeplot(distribution, cumulative=True,bw=.05, label=&quot;Learned Value&quot;)
sns.kdeplot(rewards, cumulative=True, bw=.05, label=&quot;Reward (GT)&quot;)
xlabel(&quot;Reward (Learned Value)&quot;)
ylabel(&quot;Cumulative probability&quot;)
 
subplot(1,2,2) # 累積分布
plot(tau, distribution)
xlabel(&quot;Asymmetric scaling factors (&quot;+ r&quot;$\tau$)&quot;)
ylabel(&quot;Learned Value&quot;)
tight_layout()
show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/distributional-td-learning_20_0.png" src="../_images/distributional-td-learning_20_0.png" />
</div>
</div>
</div>
<div class="section" id="signdistributional-rl">
<h3>sign関数を用いたDistributional RLと分位点回帰<a class="headerlink" href="#signdistributional-rl" title="Permalink to this headline">#</a></h3>
<p>それでは，なぜ予測価値 <span class="math notranslate nohighlight">\(V_i\)</span>は<span class="math notranslate nohighlight">\(\tau_i\)</span> 分位点に収束するのでしょうか．Extended Data Fig.1のように平衡点で考えてもよいのですが，後のために分位点回帰との関連について説明します．分位点回帰については記事を書いたので先にそちらを読んでもらうと分かりやすいと思います (→<a class="reference external" href="https://salad-bowl-of-knowledge.github.io/hp/statistics/2020/01/21/quantile_regression.html">Pythonによる分位点回帰 (Quantile regression)</a>)．</p>
<p>実はDistributional RL (かつ，RPEの応答関数にsign関数を用いた場合)における予測報酬 <span class="math notranslate nohighlight">\(V_i\)</span>の更新式は，分位点回帰(Quantile
regression)を勾配法で行うときの更新式とほとんど同じです．分位点回帰では<span class="math notranslate nohighlight">\(\delta\)</span>の関数<span class="math notranslate nohighlight">\(\rho_{\tau}(\delta)\)</span>を次のように定義します． $<span class="math notranslate nohighlight">\( \rho_{\tau}(\delta)=\left|\tau-\mathbb{I}_{\delta \leq 0}\right|\cdot |\delta|=\left(\tau-\mathbb{I}_{\delta
\leq 0}\right)\cdot \delta \)</span><span class="math notranslate nohighlight">\( そして，この関数を最小化することで回帰を行います．ここで\)</span>\tau<span class="math notranslate nohighlight">\(は分位点です．また\)</span>\delta=r-V<span class="math notranslate nohighlight">\(としておきます．今回，どんな行動をしても未来の報酬に影響はないので\)</span>\gamma=0<span class="math notranslate nohighlight">\(としています．&lt;br/&gt;
&lt;br/&gt;
ここで， \)</span><span class="math notranslate nohighlight">\( \frac{\partial \rho_{\tau}(\delta)}{\partial \delta}=\rho_{\tau}^{\prime}(\delta)=\left|\tau-\mathbb{I}_{\delta \leq 0}\right| \cdot \operatorname{sign}(\delta) \)</span><span class="math notranslate nohighlight">\( なので，\)</span>r<span class="math notranslate nohighlight">\(を観測値とすると， \)</span><span class="math notranslate nohighlight">\(
\frac{\partial \rho_{\tau}(\delta)}{\partial V}=\frac{\partial \rho_{\tau}(\delta)}{\partial \delta}\frac{\partial \delta(V)}{\partial V}=-\left|\tau-\mathbb{I}_{\delta \leq 0}\right| \cdot
\operatorname{sign}(\delta) \)</span><span class="math notranslate nohighlight">\( となります．ゆえに\)</span>V<span class="math notranslate nohighlight">\(の更新式は \)</span><span class="math notranslate nohighlight">\( V \leftarrow V - \beta\cdot\frac{\partial \rho_{\tau}(\delta)}{\partial V}=V+\beta \left|\tau-\mathbb{I}_{\delta \leq 0}\right| \cdot
\operatorname{sign}(\delta) \)</span><span class="math notranslate nohighlight">\( です．ただし，\)</span>\beta<span class="math notranslate nohighlight">\(はベースラインの学習率です．個々の\)</span>V_i<span class="math notranslate nohighlight">\(について考え，符号で場合分けをすると
\)</span><span class="math notranslate nohighlight">\( \begin{cases} V_{i} \leftarrow V_{i}+\beta\cdot |\tau_i|\cdot\operatorname{sign}\left(\delta_{i}\right)
&amp;\text { for } \delta_{i}&gt;0\\ V_{i} \leftarrow V_{i}+\beta\cdot |\tau_i-1|\cdot\operatorname{sign}\left(\delta_{i}\right) &amp;\text { for } \delta_{i} \leq 0 \end{cases} \)</span><span class="math notranslate nohighlight">\( となります．\)</span>0 \leq
\tau_i \leq 1<span class="math notranslate nohighlight">\(であり，\)</span>\tau_i=\alpha_{i}^{+} / \left(\alpha_{i}^{+} + \alpha_{i}^{-}\right)<span class="math notranslate nohighlight">\(であることに注意すると上式は次のように書けます． \)</span><span class="math notranslate nohighlight">\( \begin{cases} V_{i} \leftarrow V_{i}+\beta\cdot
\frac{\alpha_{i}^{+}}{\alpha_{i}^{+}+\alpha_{i}^{-}}\cdot\operatorname{sign}\left(\delta_{i}\right) &amp;\text { for } \delta_{i}&gt;0\\ V_{i} \leftarrow V_{i}+\beta\cdot
\frac{\alpha_{i}^{-}}{\alpha_{i}^{+}+\alpha_{i}^{-}}\cdot\operatorname{sign}\left(\delta_{i}\right) &amp;\text { for } \delta_{i} \leq 0 \end{cases} \)</span>$ これは前節で述べたDistributional
RLの更新式とほぼ同じです．いくつか違う点もありますが，RPEが正の場合と負の場合に更新される値の比は同じとなっています．</p>
<p>このようにRPEの応答関数にsign関数を用いた場合，報酬分布を上手く符号化することができます．しかし実際のドパミンニューロンはsign関数のような生理的に妥当でない応答はせず，RPEの大きさに応じた活動をします．そこで次節ではRPEの応答関数を線形にしたときの話をします．</p>
</div>
</div>
<div class="section" id="expectile-decoding">
<h2>12.2.3 Expectile モデルとドパミンニューロンからの報酬分布のDecoding<a class="headerlink" href="#expectile-decoding" title="Permalink to this headline">#</a></h2>
<div class="section" id="rpeexpectile">
<h3>RPEに対する応答が線形なモデルとExpectile回帰<a class="headerlink" href="#rpeexpectile" title="Permalink to this headline">#</a></h3>
<p>節の最後で述べたようにドパミンニューロンの活動はsign関数ではなく線形な応答をする，とした方が生理学的に妥当である (発火率を表現するならば<span class="math notranslate nohighlight">\(f(\delta)=c+\delta\quad(c &gt; 0)\)</span>とした方が良いだろうが)．それでは予測価値の更新式を</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
\begin{cases} V_{i}(x) \leftarrow V_{i}(x)+\alpha_{i}^{+}
\delta_{i} &amp;\text{for } \delta_{i} \gt 0\\ V_{i}(x) \leftarrow V_{i}(x)+\alpha_{i}^{-} \delta_{i} &amp;\text{for } \delta_{i} \leq 0 \end{cases} 
\end{split}\]</div>
<p>とした場合は，分位点回帰ではなく何に対応するのだろうか．結論から言えば，この場合は <strong>エクスペクタイル回帰(Expectile
regression)</strong> と同じになる．expectileという用語自体はexpectationとquantileを合わせたような概念，というところから来ている．<strong>中央値(median)に対する分位数(quantile)が，平均(mean)あるいは期待値(expectation)に対するexpectileの関係と同じ</strong> であると捉えると良いです．
もう少し言えば，前者は誤差のL1ノルム, 後者はL2ノルムの損失関数を最小化することにより得られる (cf. <a class="reference external" href="https://freakonometrics.hypotheses.org/files/2017/05/erasmus-1.pdf">Quantile and Expectile Regressions</a>)．</p>
<p>分位点回帰で用いた損失関数は</p>
<div class="math notranslate nohighlight">
\[
\rho_{\tau}(\delta)=\left|\tau-\mathbb{I}_{\delta \leq 0}\right|\cdot |\delta|
\]</div>
<p>だったが，最後の<span class="math notranslate nohighlight">\(|\delta|\)</span>を<span class="math notranslate nohighlight">\(\delta^2\)</span>として，</p>
<div class="math notranslate nohighlight">
\[
\rho^E_{\tau}(\delta)=\left|\tau-\mathbb{I}_{\delta \leq
0}\right|\cdot \delta^2
\]</div>
<p>とする．これを微分すれば</p>
<div class="math notranslate nohighlight">
\[ 
\frac{\partial \rho^E_{\tau}(\delta)}{\partial \delta}=\rho_{\tau}^{E\prime}(\delta)=2 \cdot \left|\tau-\mathbb{I}_{\delta \leq 0}\right| \cdot \delta 
\]</div>
<p>となり，上記の予測価値の更新式がExpectile回帰の損失関数から導けることが分かる．</p>
</div>
<div class="section" id="decoding">
<h3>報酬分布のデコーディング (decoding)<a class="headerlink" href="#decoding" title="Permalink to this headline">#</a></h3>
<p>それで，RPEの応答を線形とした場合は報酬分布を上手く学習できるのかという話ですが，実はRPEの応答をsign関数とした場合と同じように学習後の予測価値の分布を求めても報酬分布は復元されません (簡単な修正で確認できます)．そこで報酬分布をデコーディングする方法を考えます．</p>
<p>デコーデイングには各細胞が学習した予測価値(またはreversal points) <span class="math notranslate nohighlight">\(V_i\)</span>, asymmetries <span class="math notranslate nohighlight">\(\tau_i\)</span>, および報酬分布(ただし報酬の下限と上限からの一様分布)からのサンプル <span class="math notranslate nohighlight">\(z_m (m=1,2,\cdots,
M)\)</span>を用います．<span class="math notranslate nohighlight">\(N\)</span>を推定する<span class="math notranslate nohighlight">\(V_i\)</span>の数，<span class="math notranslate nohighlight">\(M=100\)</span>を1つの報酬サンプル集合<span class="math notranslate nohighlight">\(\{z_m\}\)</span>内の要素数としたとき，次の損失関数を最小にする集合<span class="math notranslate nohighlight">\(\{z_m\}\)</span>を求めます． $<span class="math notranslate nohighlight">\( \mathcal{L}(z, V, \tau)=\frac{1}{M} \sum_{m-1}^{M} \sum_{n=1}^{N}\left|\tau_{n}-\mathbb{I}_{z_{m} \leq
V_{n}}\right|\left(z_{m}-V_{n}\right)^{2} \)</span><span class="math notranslate nohighlight">\( ここで，集合\)</span>{z_m}<span class="math notranslate nohighlight">\(は20000回サンプリングするとします．損失関数\)</span>\mathcal{L}$を最小化する集合の分布が推定された報酬分布となっているので，それをplotします．以下はその結果とコードです
(このコードはほとんど著者実装のままです)．灰色が元の報酬分布で，紫がデコーデイングされた分布です．完全とはいきませんが，ある程度は推定できていることが分かります．</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c">#collapse-hide</span>

<span class="k">import</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span>
<span class="k">import</span> <span class="n">scipy</span><span class="o">.</span><span class="n">optimize</span>
 
<span class="n">def</span> <span class="n">expectile_loss_fn</span><span class="p">(</span><span class="n">expectiles</span><span class="p">,</span> <span class="n">taus</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span><span class="o">:</span>
  <span class="s">&quot;&quot;&quot;Expectile loss function, corresponds to distributional TD model &quot;&quot;&quot;</span>
  <span class="c"># distributional TD model: delta_t = (r + \gamma V*) - V_i</span>
  <span class="c"># expectile loss: delta = sample - expectile</span>
  <span class="n">delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="n">None</span><span class="p">,</span> <span class="o">:</span><span class="p">]</span> <span class="o">-</span> <span class="n">expectiles</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">None</span><span class="p">])</span>
 
  <span class="c"># distributional TD model: alpha^+ delta if delta &gt; 0, alpha^- delta otherwise</span>
  <span class="c"># expectile loss: |taus - I_{delta &lt;= 0}| * delta^2</span>
 
  <span class="c"># Note: When used to decode we take the gradient of this loss,</span>
  <span class="c"># and then evaluate the mean-squared gradient. That is because *samples* must</span>
  <span class="c"># trade-off errors with all expectiles to zero out the gradient of the </span>
  <span class="c"># expectile loss.</span>
  <span class="n">indic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">delta</span> <span class="o">&lt;=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">grad</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">taus</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">indic</span><span class="p">)</span> <span class="o">*</span> <span class="n">delta</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)))</span>
 
<span class="n">def</span> <span class="n">run_decoding</span><span class="p">(</span><span class="n">reversal_points</span><span class="p">,</span> <span class="n">taus</span><span class="p">,</span> <span class="n">minv</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">maxv</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>
                 <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">:</span>
  <span class="s">&quot;&quot;&quot;Run decoding given reversal points and asymmetries (taus).&quot;&quot;&quot;</span>
   
  <span class="c"># sort</span>
  <span class="n">ind</span> <span class="o">=</span> <span class="n">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">reversal_points</span><span class="p">))</span>
  <span class="n">points</span> <span class="o">=</span> <span class="n">reversal_points</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
  <span class="n">tau</span> <span class="o">=</span> <span class="n">taus</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
 
  <span class="c"># Robustified optimization to infer distribution</span>
  <span class="c"># Generate max_epochs sets of samples,</span>
  <span class="c"># each starting the optimization at the best of max_samples initial points.</span>
  <span class="n">sampled_dist</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">_</span> <span class="k">in</span> <span class="n">range</span><span class="p">(</span><span class="n">max_epochs</span><span class="p">)</span><span class="o">:</span>
      <span class="c"># Randomly search for good initial conditions</span>
      <span class="c"># This significantly improves the minima found</span>
      <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">minv</span><span class="p">,</span> <span class="n">maxv</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">max_samples</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>
      <span class="n">fvalues</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">expectile_loss_fn</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span> <span class="k">for</span> <span class="n">x0</span> <span class="k">in</span> <span class="n">samples</span><span class="p">])</span>
 
      <span class="c"># Perform loss minimizing on expectile loss (w.r.t samples)</span>
      <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sorted</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="n">fvalues</span><span class="o">.</span><span class="n">argmin</span><span class="p">()]))</span>
      <span class="n">fn_to_minimize</span> <span class="o">=</span> <span class="n">lambda</span> <span class="n">x</span><span class="o">:</span> <span class="n">expectile_loss_fn</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
              <span class="n">fn_to_minimize</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>
              <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="n">minv</span><span class="p">,</span> <span class="n">maxv</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="k">in</span> <span class="n">x0</span><span class="p">],</span> <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">)[</span><span class="s">&quot;x&quot;</span><span class="p">]</span>
      <span class="n">sampled_dist</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
 
  <span class="k">return</span> <span class="n">sampled_dist</span><span class="p">,</span> <span class="n">expectile_loss_fn</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sampled_dist</span><span class="p">))</span>
 
 
<span class="c"># reward distribution</span>
<span class="n">juice_amounts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
<span class="n">juice_empirical_probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[</span><span class="mf">0.06612594</span><span class="p">,</span> <span class="mf">0.09090909</span><span class="p">,</span> <span class="mf">0.14847358</span><span class="p">,</span> <span class="mf">0.15489467</span><span class="p">,</span>
     <span class="mf">0.31159175</span><span class="p">,</span> <span class="mf">0.1509519</span> <span class="p">,</span> <span class="mf">0.07705306</span><span class="p">])</span>
 
<span class="c"># samples of reward (1000, )</span>
<span class="n">sampled_empirical_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
    <span class="n">juice_amounts</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">juice_empirical_probs</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
 
<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">10</span> <span class="c"># num of simulation trial</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">20000</span> <span class="c"># num of simulation epoch</span>
<span class="n">num_cells</span> <span class="o">=</span> <span class="mi">151</span>  <span class="c"># num of cells or units</span>
<span class="n">n_decodings</span> <span class="o">=</span> <span class="mi">5</span> <span class="c"># num of decodings</span>
 
<span class="c"># Global scale for learning rates</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">0.2</span>
 
<span class="c"># Distributional TD simulation and decoding</span>
<span class="n">distribution</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">num_cells</span><span class="p">))</span>
<span class="n">alpha_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">num_cells</span><span class="p">))</span><span class="o">*</span><span class="n">beta</span>
<span class="n">alpha_neg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">num_cells</span><span class="p">))</span><span class="o">*</span><span class="n">beta</span> 
<span class="c"># alpha_neg = beta - alpha_pos としてもよい</span>
 
<span class="c"># Simulation</span>
<span class="k">for</span> <span class="n">trial</span> <span class="k">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">))</span><span class="o">:</span>
    <span class="k">for</span> <span class="n">step</span> <span class="k">in</span> <span class="n">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">)</span><span class="o">:</span>
        <span class="c"># Sample reward</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">juice_amounts</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">juice_empirical_probs</span><span class="p">)</span>
        <span class="c"># Compute TD error</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">-</span> <span class="n">distribution</span><span class="p">[</span><span class="n">trial</span><span class="p">]</span>
        <span class="c"># Update distributional value estimate</span>
        <span class="n">valence</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">delta</span> <span class="o">&lt;=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">valence</span> <span class="o">*</span> <span class="n">alpha_neg</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">valence</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha_pos</span>
        <span class="n">distribution</span><span class="p">[</span><span class="n">trial</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">delta</span>
 
<span class="c"># Decoding from distributional TD (DTD) simulation</span>
<span class="n">dtd_samples</span> <span class="o">=</span> <span class="p">[]</span> <span class="c"># </span>
<span class="n">dtd_losses</span> <span class="o">=</span> <span class="p">[]</span> <span class="c"># decoding loss</span>
<span class="n">taus</span> <span class="o">=</span> <span class="n">alpha_pos</span> <span class="o">/</span> <span class="p">(</span><span class="n">alpha_pos</span> <span class="o">+</span> <span class="n">alpha_neg</span><span class="p">)</span>
 
<span class="n">asym_variance</span> <span class="o">=</span> <span class="mf">0.2</span>
 
<span class="k">for</span> <span class="n">t</span> <span class="k">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">range</span><span class="p">(</span><span class="n">n_decodings</span><span class="p">))</span><span class="o">:</span>
    <span class="c"># Add noise to the scaling, but have mean 0.5 giving symmetric updates</span>
    <span class="n">scaling_noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">len</span><span class="p">(</span><span class="n">taus</span><span class="p">)))</span> <span class="o">*</span> <span class="n">asym_variance</span>
    <span class="n">noisy_tau</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">taus</span> <span class="o">+</span> <span class="n">scaling_noise</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span> <span class="c"># add noise</span>
 
    <span class="c"># Run decoding for distributional TD</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">run_decoding</span><span class="p">(</span>
      <span class="n">distribution</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">noisy_tau</span><span class="p">,</span> 
      <span class="n">minv</span><span class="o">=</span><span class="n">juice_amounts</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">maxv</span><span class="o">=</span><span class="n">juice_amounts</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span>
      <span class="n">max_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_samples</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">&quot;TNC&quot;</span><span class="p">)</span>
 
    <span class="n">dtd_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">dtd_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="c"># print(t, values[1]) </span>
 
<span class="c"># results of decoding</span>
<span class="n">dtd_reward_decode</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtd_samples</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
 
<span class="c"># plot</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="c"># Ground truth</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">sampled_empirical_dist</span><span class="p">,</span> <span class="n">bw</span><span class="o">=</span><span class="mf">.75</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&quot;k&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">shade</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">rugplot</span><span class="p">(</span><span class="n">sampled_empirical_dist</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;Empirical&quot;</span><span class="p">)</span>
 
<span class="c"># decoded distribution</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">dtd_reward_decode</span><span class="p">,</span> <span class="n">bw</span><span class="o">=</span><span class="mf">.75</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">plasma</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mf">4.</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shade</span><span class="o">=</span><span class="n">False</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">rugplot</span><span class="p">(</span><span class="n">dtd_reward_decode</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">plasma</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;Decoded&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">draw</span> <span class="k">in</span> <span class="n">dtd_samples</span><span class="o">:</span>
  <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">draw</span><span class="p">,</span> <span class="n">bw</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">plasma</span><span class="p">(</span><span class="mf">0.</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">shade</span><span class="o">=</span><span class="n">False</span><span class="p">)</span>
 
<span class="n">tick_params</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="n">False</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="n">False</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">&quot;best&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;Reward&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;Density&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">title</span><span class="p">(</span><span class="s">&quot;Distributional TD Decoding&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">tight_layout</span><span class="p">()</span>
<span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:05&lt;00:00,  1.81it/s]
100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:33&lt;00:00,  6.69s/it]
</pre></div>
</div>
<img alt="../_images/distributional-td-learning_25_1.png" src="../_images/distributional-td-learning_25_1.png" />
</div>
</div>
<p>このようにしてRPEに対する応答が線形であるとした場合でも報酬分布を推定できました．同じことを著者らはドパミンニューロンの活動に対しても行い，報酬分布がデコーデイングされることを示しています．ただ，デコーデイングの手間が結構かかっている気がするので，学習した予測価値分布を利用するときにはどのような処理をしているのかは気になります．</p>
</div>
</div>
<div class="section" id="id2">
<h2>参考文献<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Dabney, W., Kurth-Nelson, Z., Uchida, N. <em>et al.</em> A distributional code for value in dopamine-based reinforcement learning. <em>Nature</em> (2020). <a class="reference external" href="https://www.nature.com/articles/s41586-019-1924-6">https://doi.org/10.1038/s41586-019-1924-6</a></p></li>
<li><p>Watabe-Uchida, M. et al. Whole-Brain Mapping of Direct Inputs to Midbrain Dopamine Neurons. Neuron 74, 5, 858 - 873 (2012). <a class="reference external" href="https://www.cell.com/neuron/fulltext/S0896-6273(12)00281-4">https://doi.org/10.1016/j.neuron.2012.03.017</a><a class="reference external" href="https://www.cell.com/neuron/fulltext/S0896-6273(12)00281-4"> </a></p></li>
<li><p>Eshel, N., Tian, J., Bukwich, M. <em>et al.</em> Dopamine neurons share common response function for reward prediction error. <em>Nat Neurosci</em> <strong>19,</strong> 479–486 (2016). <a class="reference external" href="https://www.nature.com/articles/nn.4239">https://doi.org/10.1038/nn.4239</a></p></li>
<li><p>Schultz, W., Dayan, P., Montague, P.R. A neural substrate of prediction and reward. <em>Science</em>. 275, 1593-9 (1997). <a class="reference external" href="https://science.sciencemag.org/content/275/5306/1593.long">doi:10.1126/science.275.5306.1593</a></p></li>
<li><p>Chang, C., Esber, G., Marrero-Garcia, Y. <em>et al.</em> Brief optogenetic inhibition of dopamine neurons mimics endogenous negative reward prediction errors. <em>Nat Neurosci</em> <strong>19,</strong> 111–116 (2016) <a class="reference external" href="https://www.nature.com/articles/nn.4191">doi:10.1038/nn.4191</a></p></li>
<li><p>Bayer, H.M., Lau, B., Glimcher, P.W. Statistics of midbrain dopamine neuron spike trains in the awake primate. <em>J Neurophysiol</em>. <strong>98</strong>(3):1428-39 (2007). <a class="reference external" href="https://www.physiology.org/doi/full/10.1152/jn.01140.2006">https://doi.org/10.1152/jn.01140.2006</a></p></li>
<li><p>Eshel, N., Bukwich, M., Rao, V. <em>et al.</em> Arithmetic and local circuitry underlying dopamine prediction errors. <em>Nature</em> <strong>525,</strong> 243–246 (2015). <a class="reference external" href="https://www.nature.com/articles/nature14855">https://doi.org/10.1038/nature14855</a></p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "julia-1.8"
        },
        kernelOptions: {
            kernelName: "julia-1.8",
            path: "./bayesian-brain"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'julia-1.8'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Takuto Yamamoto<br/>
  
      &copy; Copyright 2020-2021.<br/>
    <div class="extra_footer">
      <div>
<script src="https://utteranc.es/client.js"
repo="compneuro-julia/compneuro-julia.github.io"
issue-term="title"
label="💬"
theme="github-light"
crossorigin="anonymous"
async>
</script>
</div>

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>